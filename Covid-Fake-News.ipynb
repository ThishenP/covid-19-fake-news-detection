{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "659902f1-83fb-48a6-a872-554e71f93735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5c7080bd-9360-46ee-9ae7-24f3aaae8c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5; 1/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 5/5; 1/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.5;, score=0.507 total time=   0.3s\n",
      "[CV 5/5; 3/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 5/5; 3/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=1.0;, score=0.507 total time=   0.6s\n",
      "[CV 1/5; 6/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 1/5; 6/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=1.0;, score=0.986 total time=   3.0s\n",
      "[CV 4/5; 8/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 4/5; 8/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.75;, score=0.970 total time=   5.4s\n",
      "[CV 1/5; 14/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 1/5; 14/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.75;, score=0.984 total time=   2.6s\n",
      "[CV 4/5; 16/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 4/5; 16/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.5;, score=0.970 total time=   3.2s\n",
      "[CV 1/5; 19/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 1/5; 19/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.5;, score=0.975 total time=   0.3s\n",
      "[CV 2/5; 19/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 2/5; 19/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.5;, score=0.994 total time=   0.3s\n",
      "[CV 3/5; 19/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 3/5; 19/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.5;, score=0.975 total time=   0.3s\n",
      "[CV 5/5; 19/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 5/5; 19/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.5;, score=0.978 total time=   0.3s\n",
      "[CV 2/5; 20/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 2/5; 20/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.75;, score=0.980 total time=   0.6s\n",
      "[CV 4/5; 20/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 4/5; 20/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.75;, score=0.970 total time=   0.5s\n",
      "[CV 5/5; 21/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 5/5; 21/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=1.0;, score=0.978 total time=   0.6s\n",
      "[CV 1/5; 23/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 1/5; 23/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.75;, score=0.971 total time=   2.7s\n",
      "[CV 2/5; 25/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 2/5; 25/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.5;, score=0.990 total time=   3.2s\n",
      "[CV 4/5; 27/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 4/5; 27/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=1.0;, score=0.983 total time=   6.2s\n",
      "[CV 4/5; 34/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 4/5; 34/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.5;, score=0.971 total time=   3.3s\n",
      "[CV 5/5; 36/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 5/5; 36/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=1.0;, score=0.979 total time=   5.5s\n",
      "[CV 3/5; 43/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 3/5; 43/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.5;, score=0.995 total time=   3.6s\n",
      "[CV 4/5; 45/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 4/5; 45/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=1.0;, score=0.991 total time=   6.5s\n",
      "[CV 3/5; 52/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 3/5; 52/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.5;, score=0.994 total time=   3.5s\n",
      "[CV 4/5; 54/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 4/5; 54/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=1.0;, score=0.990 total time=   7.6s\n",
      "[CV 4/5; 61/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 4/5; 61/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=0.5;, score=0.989 total time=   3.7s\n",
      "[CV 5/5; 63/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 5/5; 63/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=1.0;, score=0.996 total time=   6.1s\n",
      "[CV 3/5; 70/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 3/5; 70/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=0.5;, score=0.996 total time=   3.5s\n",
      "[CV 4/5; 72/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 4/5; 72/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=1.0;, score=0.992 total time=   6.4s\n",
      "[CV 1/5; 79/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 1/5; 79/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=0.5;, score=0.960 total time=   4.2s\n",
      "[CV 5/5; 81/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 5/5; 81/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=1.0;, score=0.991 total time=   4.1s\n",
      "[CV 1/5; 87/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 1/5; 87/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=1.0;, score=0.988 total time=   3.5s\n",
      "[CV 1/5; 90/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 1/5; 90/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=1.0;, score=0.985 total time=   5.7s\n",
      "\n",
      "[CV 5/5; 79/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=0.5;, score=0.993 total time=   3.7s\n",
      "[CV 2/5; 82/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 2/5; 82/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=0.5;, score=0.988 total time=   0.3s\n",
      "[CV 3/5; 82/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 3/5; 82/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=0.5;, score=0.982 total time=   0.4s\n",
      "[CV 5/5; 82/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 5/5; 82/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=0.5;, score=0.989 total time=   0.5s\n",
      "[CV 3/5; 83/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 3/5; 83/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=0.75;, score=0.991 total time=   0.6s\n",
      "[CV 3/5; 84/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 3/5; 84/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=1.0;, score=0.988 total time=   0.8s\n",
      "[CV 1/5; 86/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 1/5; 86/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=0.75;, score=0.989 total time=   2.4s\n",
      "[CV 2/5; 88/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 2/5; 88/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=0.5;, score=0.995 total time=   3.4s\n",
      "[CV 3/5; 90/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 3/5; 90/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=1.0;, score=0.994 total time=   5.0s\n",
      "[CV 2/5; 2/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 2/5; 2/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.75;, score=0.508 total time=   0.5s\n",
      "[CV 4/5; 4/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 4/5; 4/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.5;, score=0.970 total time=   1.6s\n",
      "[CV 5/5; 6/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 5/5; 6/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=1.0;, score=0.978 total time=   2.8s\n",
      "[CV 2/5; 9/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 2/5; 9/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=1.0;, score=0.980 total time=   5.7s\n",
      "[CV 4/5; 15/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 4/5; 15/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=1.0;, score=0.970 total time=   3.1s\n",
      "[CV 1/5; 18/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 1/5; 18/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=1.0;, score=0.986 total time=   5.7s\n",
      "[CV 3/5; 23/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 3/5; 23/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.75;, score=0.975 total time=   2.1s\n",
      "[CV 4/5; 25/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 4/5; 25/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.5;, score=0.984 total time=   3.2s\n",
      "[CV 1/5; 28/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 1/5; 28/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.5;, score=0.971 total time=   0.3s\n",
      "[CV 2/5; 28/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 2/5; 28/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.5;, score=0.980 total time=   0.3s\n",
      "[CV 3/5; 28/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 3/5; 28/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.5;, score=0.993 total time=   0.5s\n",
      "[CV 1/5; 29/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 1/5; 29/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.75;, score=0.983 total time=   0.6s\n",
      "[CV 4/5; 29/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 4/5; 29/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.75;, score=0.970 total time=   0.5s\n",
      "[CV 2/5; 31/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 2/5; 31/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.5;, score=0.979 total time=   1.6s\n",
      "[CV 3/5; 33/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 3/5; 33/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=1.0;, score=0.975 total time=   3.1s\n",
      "[CV 1/5; 36/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 1/5; 36/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=1.0;, score=0.971 total time=   5.8s\n",
      "[CV 1/5; 42/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 1/5; 42/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=1.0;, score=0.988 total time=   2.7s\n",
      "[CV 3/5; 44/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 3/5; 44/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.75;, score=0.994 total time=   5.6s\n",
      "[CV 5/5; 49/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 5/5; 49/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.5;, score=0.995 total time=   1.5s\n",
      "[CV 3/5; 50/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 3/5; 50/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.75;, score=0.994 total time=   2.7s\n",
      "[CV 5/5; 52/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 5/5; 52/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.5;, score=0.995 total time=   3.6s\n",
      "[CV 2/5; 55/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 2/5; 55/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=0.5;, score=0.992 total time=   0.4s\n",
      "[CV 4/5; 55/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 4/5; 55/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=0.5;, score=0.989 total time=   0.3s\n",
      "[CV 5/5; 55/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 5/5; 55/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=0.5;, score=0.990 total time=   0.3s\n",
      "[CV 2/5; 56/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 2/5; 56/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=0.75;, score=0.995 total time=   0.5s\n",
      "[CV 4/5; 56/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 4/5; 56/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=0.75;, score=0.988 total time=   0.7s\n",
      "[CV 1/5; 58/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 1/5; 58/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=0.5;, score=0.982 total time=   2.0s\n",
      "[CV 4/5; 59/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 4/5; 59/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=0.75;, score=0.985 total time=   2.6s\n",
      "[CV 2/5; 62/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 2/5; 62/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=0.75;, score=0.996 total time=   5.1s\n",
      "[CV 1/5; 66/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 1/5; 66/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=1.0;, score=0.987 total time=   0.8s\n",
      "[CV 2/5; 67/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 2/5; 67/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=0.5;, score=0.996 total time=   2.2s\n",
      "[CV 4/5; 69/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 4/5; 69/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=1.0;, score=0.992 total time=   3.5s\n",
      "[CV 2/5; 72/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 2/5; 72/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=1.0;, score=0.996 total time=   6.5s\n",
      "[CV 2/5; 78/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 2/5; 78/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=1.0;, score=0.995 total time=   2.6s\n",
      "[CV 4/5; 80/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 4/5; 80/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=0.75;, score=0.990 total time=   5.9s\n",
      "[CV 3/5; 86/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 3/5; 86/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=0.75;, score=0.996 total time=   3.3s\n",
      "[CV 1/5; 89/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 1/5; 89/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=0.75;, score=0.988 total time=   5.3s\n",
      "[CV 1/5; 2/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 1/5; 2/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.75;, score=0.507 total time=   0.5s\n",
      "[CV 1/5; 5/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 1/5; 5/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.75;, score=0.983 total time=   2.3s\n",
      "[CV 4/5; 7/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 4/5; 7/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.5;, score=0.970 total time=   3.3s\n",
      "[CV 1/5; 10/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 1/5; 10/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.5;, score=0.507 total time=   0.3s\n",
      "[CV 2/5; 10/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 2/5; 10/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.5;, score=0.508 total time=   0.3s\n",
      "[CV 4/5; 10/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 4/5; 10/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.5;, score=0.507 total time=   0.3s\n",
      "[CV 1/5; 11/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 1/5; 11/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.75;, score=0.507 total time=   0.4s\n",
      "[CV 3/5; 11/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 3/5; 11/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.75;, score=0.507 total time=   0.5s\n",
      "[CV 2/5; 12/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 2/5; 12/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=1.0;, score=0.508 total time=   0.6s\n",
      "[CV 4/5; 13/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 4/5; 13/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.5;, score=0.970 total time=   1.7s\n",
      "[CV 3/5; 15/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 3/5; 15/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=1.0;, score=0.975 total time=   2.7s\n",
      "[CV 3/5; 17/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 3/5; 17/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.75;, score=0.975 total time=   3.9s\n",
      "[CV 1/5; 21/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 1/5; 21/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=1.0;, score=0.986 total time=   0.6s\n",
      "[CV 1/5; 22/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 1/5; 22/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.5;, score=0.981 total time=   2.0s\n",
      "[CV 2/5; 24/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 2/5; 24/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=1.0;, score=0.994 total time=   2.7s\n",
      "[CV 5/5; 26/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 5/5; 26/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.75;, score=0.990 total time=   4.7s\n",
      "[CV 5/5; 30/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 5/5; 30/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=1.0;, score=0.978 total time=   0.6s\n",
      "[CV 1/5; 32/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 1/5; 32/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.75;, score=0.971 total time=   2.3s\n",
      "[CV 3/5; 34/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 3/5; 34/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.5;, score=0.979 total time=   2.9s\n",
      "[CV 4/5; 36/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 4/5; 36/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=1.0;, score=0.970 total time=   5.4s\n",
      "[CV 5/5; 42/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 5/5; 42/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=1.0;, score=0.995 total time=   2.8s\n",
      "[CV 5/5; 44/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 5/5; 44/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.75;, score=0.996 total time=   5.2s\n",
      "[CV 1/5; 50/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 1/5; 50/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.75;, score=0.986 total time=   2.8s\n",
      "[CV 4/5; 52/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 4/5; 52/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.5;, score=0.990 total time=   4.2s\n",
      "[CV 5/5; 54/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 5/5; 54/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=1.0;, score=0.995 total time=   6.9s\n",
      "[CV 3/5; 61/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 3/5; 61/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=0.5;, score=0.990 total time=   3.5s\n",
      "[CV 4/5; 63/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 4/5; 63/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=1.0;, score=0.990 total time=   7.3s\n",
      "[CV 5/5; 70/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 5/5; 70/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=0.5;, score=0.995 total time=   3.9s\n",
      "[CV 3/5; 73/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 3/5; 73/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=0.5;, score=0.992 total time=   0.3s\n",
      "[CV 4/5; 73/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 4/5; 73/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=0.5;, score=0.984 total time=   0.3s\n",
      "[CV 1/5; 74/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 1/5; 74/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=0.75;, score=0.985 total time=   0.5s\n",
      "[CV 3/5; 74/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 3/5; 74/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=0.75;, score=0.994 total time=   0.6s\n",
      "[CV 1/5; 75/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 1/5; 75/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=1.0;, score=0.987 total time=   0.8s\n",
      "[CV 3/5; 76/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 3/5; 76/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=0.5;, score=0.989 total time=   1.6s\n",
      "[CV 5/5; 77/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 5/5; 77/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=0.75;, score=0.995 total time=   2.6s\n",
      "[CV 3/5; 80/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 3/5; 80/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=0.75;, score=0.992 total time=   5.1s\n",
      "[CV 2/5; 85/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 2/5; 85/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=0.5;, score=0.993 total time=   1.8s\n",
      "[CV 3/5; 87/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 3/5; 87/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=1.0;, score=0.992 total time=   3.2s\n",
      "[CV 3/5; 89/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 3/5; 89/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=0.75;, score=0.995 total time=   4.9s\n",
      "[CV 1/5; 1/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 1/5; 1/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.5;, score=0.507 total time=   0.3s\n",
      "[CV 4/5; 3/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 4/5; 3/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=1.0;, score=0.507 total time=   0.6s\n",
      "[CV 2/5; 6/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 2/5; 6/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=1.0;, score=0.980 total time=   2.9s\n",
      "[CV 3/5; 8/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 3/5; 8/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.75;, score=0.975 total time=   4.2s\n",
      "[CV 3/5; 12/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 3/5; 12/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=1.0;, score=0.507 total time=   0.5s\n",
      "[CV 1/5; 13/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 1/5; 13/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.5;, score=0.986 total time=   1.8s\n",
      "[CV 2/5; 15/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 2/5; 15/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=1.0;, score=0.980 total time=   3.5s\n",
      "[CV 2/5; 18/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 2/5; 18/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=1.0;, score=0.980 total time=   5.9s\n",
      "[CV 5/5; 24/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 5/5; 24/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=1.0;, score=0.978 total time=   2.4s\n",
      "[CV 2/5; 26/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 2/5; 26/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.75;, score=0.990 total time=   4.5s\n",
      "[CV 5/5; 29/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 5/5; 29/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.75;, score=0.978 total time=   0.4s\n",
      "[CV 1/5; 31/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 1/5; 31/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.5;, score=0.963 total time=   1.4s\n",
      "[CV 1/5; 33/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 1/5; 33/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=1.0;, score=0.971 total time=   2.5s\n",
      "[CV 3/5; 35/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 3/5; 35/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.75;, score=0.975 total time=   5.1s\n",
      "[CV 4/5; 40/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 4/5; 40/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.5;, score=0.991 total time=   1.8s\n",
      "[CV 4/5; 42/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 4/5; 42/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=1.0;, score=0.989 total time=   3.2s\n",
      "[CV 2/5; 45/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 2/5; 45/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=1.0;, score=0.996 total time=   6.5s\n",
      "[CV 4/5; 50/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 4/5; 50/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.75;, score=0.990 total time=   3.0s\n",
      "[CV 2/5; 53/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 2/5; 53/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.75;, score=0.993 total time=   5.0s\n",
      "[CV 5/5; 56/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 5/5; 56/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=0.75;, score=0.990 total time=   0.6s\n",
      "[CV 2/5; 58/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 2/5; 58/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=0.5;, score=0.992 total time=   2.2s\n",
      "[CV 1/5; 60/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 1/5; 60/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=1.0;, score=0.987 total time=   3.3s\n",
      "[CV 3/5; 62/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 3/5; 62/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=0.75;, score=0.994 total time=   5.2s\n",
      "[CV 5/5; 67/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 5/5; 67/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=0.5;, score=0.996 total time=   1.8s\n",
      "[CV 3/5; 69/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 3/5; 69/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=1.0;, score=0.994 total time=   3.1s\n",
      "[CV 5/5; 71/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 5/5; 71/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=0.75;, score=0.996 total time=   5.8s\n",
      "[CV 2/5; 77/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 2/5; 77/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=0.75;, score=0.993 total time=   2.3s\n",
      "[CV 3/5; 79/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 3/5; 79/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=0.5;, score=0.987 total time=   3.9s\n",
      "[CV 1/5; 82/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 1/5; 82/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=0.5;, score=0.978 total time=   0.5s\n",
      "[CV 4/5; 82/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 4/5; 82/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=0.5;, score=0.990 total time=   0.5s\n",
      "[CV 1/5; 83/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 1/5; 83/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=0.75;, score=0.983 total time=   0.5s\n",
      "[CV 5/5; 83/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 5/5; 83/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=0.75;, score=0.989 total time=   0.7s\n",
      "[CV 5/5; 84/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 5/5; 84/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=1.0;, score=0.990 total time=   0.6s\n",
      "[CV 2/5; 86/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 2/5; 86/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=0.75;, score=0.996 total time=   3.0s\n",
      "[CV 5/5; 88/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 5/5; 88/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=0.5;, score=0.995 total time=   3.5s\n",
      "[CV 1/5; 3/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 1/5; 3/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=1.0;, score=0.507 total time=   0.6s\n",
      "[CV 4/5; 5/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 4/5; 5/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.75;, score=0.970 total time=   2.3s\n",
      "[CV 2/5; 8/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 2/5; 8/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.75;, score=0.980 total time=   5.0s\n",
      "[CV 4/5; 12/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 4/5; 12/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=1.0;, score=0.507 total time=   0.5s\n",
      "[CV 3/5; 13/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 3/5; 13/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.5;, score=0.975 total time=   1.7s\n",
      "[CV 1/5; 15/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 1/5; 15/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=1.0;, score=0.986 total time=   2.9s\n",
      "[CV 4/5; 17/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 4/5; 17/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.75;, score=0.970 total time=   4.8s\n",
      "[CV 5/5; 22/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 5/5; 22/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.5;, score=0.978 total time=   1.8s\n",
      "[CV 1/5; 25/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 1/5; 25/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.5;, score=0.982 total time=   2.9s\n",
      "[CV 3/5; 27/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 3/5; 27/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=1.0;, score=0.975 total time=   5.0s\n",
      "[CV 2/5; 32/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 2/5; 32/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.75;, score=0.968 total time=   2.1s\n",
      "[CV 1/5; 34/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 1/5; 34/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.5;, score=0.975 total time=   3.0s\n",
      "[CV 3/5; 36/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 3/5; 36/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=1.0;, score=0.986 total time=   5.7s\n",
      "[CV 1/5; 43/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 1/5; 43/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.5;, score=0.989 total time=   3.8s\n",
      "[CV 3/5; 45/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 3/5; 45/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=1.0;, score=0.994 total time=   6.8s\n",
      "[CV 2/5; 52/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 2/5; 52/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.5;, score=0.994 total time=   3.6s\n",
      "[CV 3/5; 54/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 3/5; 54/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=1.0;, score=0.993 total time=   7.4s\n",
      "[CV 5/5; 60/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 5/5; 60/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=1.0;, score=0.996 total time=   3.0s\n",
      "[CV 2/5; 63/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 2/5; 63/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=1.0;, score=0.993 total time=   5.9s\n",
      "[CV 4/5; 68/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 4/5; 68/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=0.75;, score=0.991 total time=   2.4s\n",
      "[CV 1/5; 71/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 1/5; 71/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=0.75;, score=0.985 total time=   5.0s\n",
      "[CV 5/5; 74/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 5/5; 74/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=0.75;, score=0.989 total time=   0.6s\n",
      "[CV 3/5; 75/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 3/5; 75/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=1.0;, score=0.992 total time=   0.6s\n",
      "[CV 4/5; 76/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 4/5; 76/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=0.5;, score=0.986 total time=   1.9s\n",
      "[CV 3/5; 78/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 3/5; 78/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=1.0;, score=0.994 total time=   3.5s\n",
      "[CV 2/5; 81/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 2/5; 81/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=1.0;, score=0.996 total time=   3.2s\n",
      "[CV 2/5; 84/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 2/5; 84/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=1.0;, score=0.989 total time=   0.8s\n",
      "[CV 3/5; 85/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 3/5; 85/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=0.5;, score=0.994 total time=   1.6s\n",
      "[CV 4/5; 87/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 4/5; 87/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=1.0;, score=0.991 total time=   3.2s\n",
      "[CV 5/5; 89/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 5/5; 89/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=0.75;, score=0.996 total time=   4.9s\n",
      "[CV 3/5; 2/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 3/5; 2/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.75;, score=0.507 total time=   0.4s\n",
      "[CV 2/5; 4/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 2/5; 4/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.5;, score=0.994 total time=   1.8s\n",
      "[CV 2/5; 7/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 2/5; 7/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.5;, score=0.980 total time=   3.5s\n",
      "[CV 4/5; 9/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 4/5; 9/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=1.0;, score=0.970 total time=   5.4s\n",
      "[CV 5/5; 15/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 5/5; 15/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=1.0;, score=0.978 total time=   2.5s\n",
      "[CV 5/5; 17/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 5/5; 17/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.75;, score=0.978 total time=   6.0s\n",
      "[CV 3/5; 24/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 3/5; 24/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=1.0;, score=0.975 total time=   2.9s\n",
      "[CV 1/5; 27/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 1/5; 27/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=1.0;, score=0.982 total time=   5.4s\n",
      "[CV 5/5; 32/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 5/5; 32/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.75;, score=0.978 total time=   2.0s\n",
      "[CV 2/5; 34/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 2/5; 34/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.5;, score=0.979 total time=   3.6s\n",
      "[CV 1/5; 37/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 1/5; 37/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.5;, score=0.981 total time=   0.3s\n",
      "[CV 3/5; 37/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 3/5; 37/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.5;, score=0.986 total time=   0.4s\n",
      "[CV 5/5; 37/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 5/5; 37/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.5;, score=0.992 total time=   0.3s\n",
      "[CV 2/5; 38/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 2/5; 38/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.75;, score=0.994 total time=   0.4s\n",
      "[CV 4/5; 38/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 4/5; 38/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.75;, score=0.983 total time=   0.4s\n",
      "[CV 2/5; 39/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 2/5; 39/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=1.0;, score=0.990 total time=   0.5s\n",
      "[CV 2/5; 40/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 2/5; 40/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.5;, score=0.994 total time=   1.8s\n",
      "[CV 4/5; 41/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 4/5; 41/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.75;, score=0.990 total time=   2.2s\n",
      "[CV 1/5; 44/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 1/5; 44/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.75;, score=0.988 total time=   5.2s\n",
      "[CV 5/5; 47/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 5/5; 47/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.75;, score=0.979 total time=   0.4s\n",
      "[CV 4/5; 48/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 4/5; 48/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=1.0;, score=0.981 total time=   0.5s\n",
      "[CV 2/5; 49/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 2/5; 49/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.5;, score=0.993 total time=   2.1s\n",
      "[CV 3/5; 51/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 3/5; 51/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=1.0;, score=0.993 total time=   3.2s\n",
      "[CV 5/5; 53/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 5/5; 53/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.75;, score=0.996 total time=   4.9s\n",
      "[CV 4/5; 57/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 4/5; 57/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=1.0;, score=0.989 total time=   0.7s\n",
      "[CV 1/5; 59/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 1/5; 59/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=0.75;, score=0.984 total time=   3.0s\n",
      "[CV 1/5; 61/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 1/5; 61/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=0.5;, score=0.984 total time=   4.7s\n",
      "[CV 3/5; 64/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 3/5; 64/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=0.5;, score=0.993 total time=   0.4s\n",
      "[CV 5/5; 64/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 5/5; 64/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=0.5;, score=0.992 total time=   0.4s\n",
      "[CV 2/5; 65/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 2/5; 65/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=0.75;, score=0.996 total time=   0.6s\n",
      "[CV 5/5; 65/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 5/5; 65/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=0.75;, score=0.989 total time=   0.6s\n",
      "[CV 4/5; 66/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 4/5; 66/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=1.0;, score=0.989 total time=   0.7s\n",
      "[CV 2/5; 68/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 2/5; 68/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=0.75;, score=0.996 total time=   2.4s\n",
      "[CV 1/5; 70/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 1/5; 70/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=0.5;, score=0.987 total time=   3.6s\n",
      "[CV 3/5; 72/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 3/5; 72/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=1.0;, score=0.995 total time=   6.0s\n",
      "[CV 5/5; 78/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 5/5; 78/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=1.0;, score=0.992 total time=   2.9s\n",
      "[CV 5/5; 80/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 5/5; 80/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=0.75;, score=0.995 total time=   5.9s\n",
      "[CV 2/5; 87/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 2/5; 87/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=1.0;, score=0.996 total time=   3.6s\n",
      "[CV 2/5; 90/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 2/5; 90/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=1.0;, score=0.995 total time=   5.9s\n",
      "[CV 4/5; 1/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 4/5; 1/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.5;, score=0.507 total time=   0.4s\n",
      "[CV 3/5; 4/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 3/5; 4/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.5;, score=0.975 total time=   1.8s\n",
      "[CV 1/5; 7/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 1/5; 7/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.5;, score=0.983 total time=   3.0s\n",
      "[CV 3/5; 9/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 3/5; 9/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=1.0;, score=0.975 total time=   6.0s\n",
      "[CV 1/5; 16/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 1/5; 16/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.5;, score=0.983 total time=   3.1s\n",
      "[CV 3/5; 18/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 3/5; 18/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=1.0;, score=0.975 total time=   5.2s\n",
      "[CV 1/5; 24/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 1/5; 24/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=1.0;, score=0.971 total time=   2.7s\n",
      "[CV 4/5; 26/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 4/5; 26/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.75;, score=0.983 total time=   4.4s\n",
      "[CV 1/5; 30/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 1/5; 30/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=1.0;, score=0.971 total time=   0.5s\n",
      "[CV 3/5; 31/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 3/5; 31/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.5;, score=0.975 total time=   1.4s\n",
      "[CV 2/5; 33/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 2/5; 33/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=1.0;, score=0.980 total time=   2.5s\n",
      "[CV 4/5; 35/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 4/5; 35/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.75;, score=0.975 total time=   4.5s\n",
      "[CV 1/5; 40/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 1/5; 40/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.5;, score=0.988 total time=   1.6s\n",
      "[CV 3/5; 41/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 3/5; 41/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.75;, score=0.994 total time=   2.4s\n",
      "[CV 5/5; 43/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 5/5; 43/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.5;, score=0.996 total time=   3.5s\n",
      "[CV 2/5; 46/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 2/5; 46/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.5;, score=0.980 total time=   0.7s\n",
      "[CV 5/5; 46/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 5/5; 46/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.5;, score=0.982 total time=   0.4s\n",
      "[CV 2/5; 47/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 2/5; 47/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.75;, score=0.980 total time=   0.4s\n",
      "[CV 4/5; 47/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 4/5; 47/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.75;, score=0.981 total time=   0.4s\n",
      "[CV 1/5; 48/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 1/5; 48/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=1.0;, score=0.971 total time=   0.5s\n",
      "[CV 5/5; 48/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 5/5; 48/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=1.0;, score=0.979 total time=   0.5s\n",
      "[CV 2/5; 50/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 2/5; 50/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.75;, score=0.993 total time=   2.4s\n",
      "[CV 1/5; 52/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 1/5; 52/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.5;, score=0.988 total time=   5.2s\n",
      "[CV 1/5; 55/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 1/5; 55/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=0.5;, score=0.989 total time=   0.5s\n",
      "[CV 3/5; 55/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 3/5; 55/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=0.5;, score=0.991 total time=   0.4s\n",
      "[CV 1/5; 56/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 1/5; 56/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=0.75;, score=0.987 total time=   0.5s\n",
      "[CV 3/5; 56/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 3/5; 56/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=0.75;, score=0.996 total time=   0.6s\n",
      "[CV 3/5; 57/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 3/5; 57/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=1.0;, score=0.994 total time=   0.8s\n",
      "[CV 2/5; 59/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 2/5; 59/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=0.75;, score=0.989 total time=   3.1s\n",
      "[CV 2/5; 61/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 2/5; 61/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=0.5;, score=0.990 total time=   3.3s\n",
      "[CV 3/5; 63/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 3/5; 63/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=1.0;, score=0.995 total time=   6.1s\n",
      "[CV 5/5; 69/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 5/5; 69/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=1.0;, score=0.995 total time=   2.9s\n",
      "[CV 1/5; 72/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 1/5; 72/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=1.0;, score=0.985 total time=   6.0s\n",
      "[CV 3/5; 77/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 3/5; 77/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=0.75;, score=0.988 total time=   2.2s\n",
      "[CV 4/5; 79/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 4/5; 79/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=0.5;, score=0.989 total time=   3.2s\n",
      "[CV 4/5; 81/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 4/5; 81/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=1.0;, score=0.986 total time=   4.4s\n",
      "[CV 5/5; 86/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 5/5; 86/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=0.75;, score=0.993 total time=   3.4s\n",
      "[CV 2/5; 89/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 2/5; 89/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=0.75;, score=0.995 total time=   5.1s\n",
      "\n",
      "[CV 4/5; 86/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=0.75;, score=0.989 total time=   2.1s\n",
      "[CV 4/5; 88/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 4/5; 88/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=0.5;, score=0.990 total time=   3.2s\n",
      "[CV 3/5; 1/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 3/5; 1/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.5;, score=0.507 total time=   0.4s\n",
      "[CV 1/5; 4/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 1/5; 4/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.5;, score=0.984 total time=   1.4s\n",
      "[CV 3/5; 6/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 3/5; 6/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=1.0;, score=0.975 total time=   2.7s\n",
      "[CV 5/5; 8/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 5/5; 8/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.75;, score=0.978 total time=   4.7s\n",
      "[CV 2/5; 14/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 2/5; 14/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.75;, score=0.980 total time=   2.5s\n",
      "[CV 3/5; 16/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 3/5; 16/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.5;, score=0.975 total time=   3.3s\n",
      "[CV 5/5; 18/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 5/5; 18/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=1.0;, score=0.978 total time=   6.1s\n",
      "[CV 3/5; 25/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 3/5; 25/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.5;, score=0.979 total time=   4.3s\n",
      "[CV 4/5; 28/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 4/5; 28/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.5;, score=0.970 total time=   0.4s\n",
      "[CV 5/5; 28/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 5/5; 28/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.5;, score=0.978 total time=   0.4s\n",
      "[CV 2/5; 29/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 2/5; 29/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.75;, score=0.994 total time=   0.4s\n",
      "[CV 3/5; 30/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 3/5; 30/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=1.0;, score=0.975 total time=   0.6s\n",
      "[CV 5/5; 31/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 5/5; 31/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.5;, score=0.977 total time=   1.6s\n",
      "[CV 5/5; 33/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 5/5; 33/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=1.0;, score=0.978 total time=   2.4s\n",
      "[CV 5/5; 35/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 5/5; 35/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.75;, score=0.979 total time=   5.1s\n",
      "[CV 2/5; 41/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 2/5; 41/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.75;, score=0.994 total time=   2.3s\n",
      "[CV 2/5; 43/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 2/5; 43/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.5;, score=0.996 total time=   3.8s\n",
      "[CV 5/5; 45/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 5/5; 45/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=1.0;, score=0.995 total time=   5.9s\n",
      "[CV 5/5; 51/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 5/5; 51/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=1.0;, score=0.993 total time=   3.2s\n",
      "[CV 2/5; 54/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 2/5; 54/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=1.0;, score=0.993 total time=   6.9s\n",
      "[CV 2/5; 60/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 2/5; 60/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=1.0;, score=0.992 total time=   3.3s\n",
      "[CV 4/5; 62/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 4/5; 62/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=0.75;, score=0.990 total time=   4.6s\n",
      "[CV 5/5; 66/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 5/5; 66/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=1.0;, score=0.994 total time=   0.6s\n",
      "[CV 1/5; 68/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 1/5; 68/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=0.75;, score=0.988 total time=   2.5s\n",
      "[CV 2/5; 70/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 2/5; 70/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=0.5;, score=0.996 total time=   3.7s\n",
      "[CV 5/5; 72/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 5/5; 72/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=1.0;, score=0.995 total time=   6.4s\n",
      "[CV 2/5; 79/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 2/5; 79/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=0.5;, score=0.990 total time=   3.7s\n",
      "[CV 3/5; 81/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 3/5; 81/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=1.0;, score=0.994 total time=   6.0s\n",
      "[CV 3/5; 88/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 3/5; 88/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=0.5;, score=0.995 total time=   3.2s\n",
      "[CV 4/5; 90/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 4/5; 90/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=1.0;, score=0.990 total time=   4.6s\n",
      "\n",
      "\n",
      "[CV 4/5; 84/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=1.0;, score=0.989 total time=   0.6s\n",
      "[CV 4/5; 85/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 4/5; 85/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=0.5;, score=0.991 total time=   2.0s\n",
      "[CV 1/5; 88/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 1/5; 88/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=0.5;, score=0.987 total time=   4.5s\n",
      "[CV 5/5; 90/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 5/5; 90/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=1.0;, score=0.996 total time=   4.3s\n"
     ]
    }
   ],
   "source": [
    "def plot_conf_matrix(y_pred, y_true, title=None):\n",
    "    \"\"\"Helper Function to plot confustion matrix\"\"\"\n",
    "    \n",
    "    conf_mat = confusion_matrix(y_pred, y_true)\n",
    "    df_cm = pd.DataFrame(conf_mat, index = [i for i in [\"Fake\", \"Real\"]],\n",
    "                  columns = [i for i in [\"Fake\", \"Real\"]])\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(df_cm, annot=True, cmap=\"Blues\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    if title:\n",
    "        plt.title(f\"{title} Confusion Matrix\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f266b5af-4823-41b0-97ce-d467abb1dbae",
   "metadata": {},
   "source": [
    "# Read in Data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44ec800a-a3b4-465e-9b74-64d29b0ceafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_news_df = pd.read_csv(\"COVID19-FNIR/COVID19-FNIR/trueNews.csv\")\n",
    "fake_news_df = pd.read_csv(\"COVID19-FNIR/COVID19-FNIR/fakeNews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "788ffbc2-3c06-4cf9-a60c-9e6b43a8d33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign class columns and combine dataframes\n",
    "fake_news_df[\"class\"] = 0\n",
    "true_news_df[\"class\"] = 1\n",
    "combined = pd.concat([fake_news_df, true_news_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "358c39f1-67fc-4981-b2db-39319c965914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset data to just contain the text field and the class field\n",
    "combined = combined[[\"Text\", \"class\"]].reset_index(drop=True)\n",
    "# Shuffle data\n",
    "combined = combined.sample(frac = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8588e879-92d7-464a-8342-5071b22b829c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>A migrant travelling home amidst the coronavi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>Hospitals treating COVID-19 in Mexican capital...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>Teen spitting in tea bottle to spread the cor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>The list shows the medicines issued by a doct...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>Italians threw away their currency on the str...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  class\n",
       "3381   A migrant travelling home amidst the coronavi...      0\n",
       "6078  Hospitals treating COVID-19 in Mexican capital...      1\n",
       "2439   Teen spitting in tea bottle to spread the cor...      0\n",
       "3737   The list shows the medicines issued by a doct...      0\n",
       "1631   Italians threw away their currency on the str...      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184b5956-a92f-4a9d-a05a-588c52a3347d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordopt(text):\n",
    "    \"\"\"Formating of the text field \"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub(\"\\\\W\",\" \",text) \n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a84ac0ad-2f60-4f1f-866f-aa62be134365",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined[\"Text\"] = combined[\"Text\"].apply(wordopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74d3e410-84a9-4c6c-9551-3ea25f69d2ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3381</th>\n",
       "      <td>a migrant travelling home amidst the coronavi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6078</th>\n",
       "      <td>hospitals treating covid  in mexican capital q...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2439</th>\n",
       "      <td>teen spitting in tea bottle to spread the cor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>the list shows the medicines issued by a doct...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1631</th>\n",
       "      <td>italians threw away their currency on the str...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text  class\n",
       "3381   a migrant travelling home amidst the coronavi...      0\n",
       "6078  hospitals treating covid  in mexican capital q...      1\n",
       "2439   teen spitting in tea bottle to spread the cor...      0\n",
       "3737   the list shows the medicines issued by a doct...      0\n",
       "1631   italians threw away their currency on the str...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd7e42c6-72c4-4ee3-9fb2-0ede20e16110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set X and y variables\n",
    "X = combined[\"Text\"]\n",
    "y = combined[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c8b19fc-1a66-4d25-81b9-52457bcd9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61573d3c-af7a-4d5e-adce-2db063ed1cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \n",
    "\n",
    "vectorization = TfidfVectorizer()\n",
    "X_train = vectorization.fit_transform(X_train)\n",
    "X_test = vectorization.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac59ee7-3e2e-4189-a87e-f4b88a413acb",
   "metadata": {},
   "source": [
    "# Model 1: Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f0d7a940-75b1-4c7d-a9bd-3c647e87f74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "\n",
    "nb_model = MultinomialNB()\n",
    "# Fit model \n",
    "nb_model.fit(X_train, y_train)\n",
    "# Predict \n",
    "y_pred = nb_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "262ad684-5594-4d6f-a163-3770d6882698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9620453347390617\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b01575a4-f621-4efd-8810-5b11ef7b2992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.94      0.96       991\n",
      "           1       0.93      0.99      0.96       906\n",
      "\n",
      "    accuracy                           0.96      1897\n",
      "   macro avg       0.96      0.96      0.96      1897\n",
      "weighted avg       0.96      0.96      0.96      1897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f0fff98-4b50-4515-8afe-29a65346b6fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_conf_matrix(y_pred, y_test, title=\"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6766beb6-fb12-4792-b9e7-cbf94a6b43f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "params = {\n",
    "    \"alpha\": [0.001, 0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1, 1.5, 2, 3, 5, 10],\n",
    "    \"fit_prior\": (True, False),\n",
    "}\n",
    "gs_clf = GridSearchCV(MultinomialNB(), params, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cdee0b08-131d-4f5a-9e53-2e80e16c0afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88ce76af-295f-4742-b7da-62ee92257730",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gs_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a75c77b0-4da1-479a-b10e-d363e7c8de38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9599367422245652\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5cd04a36-62a9-47e3-a4cc-5010578710b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.93      0.96       991\n",
      "           1       0.93      0.99      0.96       906\n",
      "\n",
      "    accuracy                           0.96      1897\n",
      "   macro avg       0.96      0.96      0.96      1897\n",
      "weighted avg       0.96      0.96      0.96      1897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "317ebf93-58bb-4fad-b283-43980c628a0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_conf_matrix(y_pred, y_test, title=\"Naive Bayes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e77592-c6e4-4a9c-845a-2f0cff3cf99e",
   "metadata": {},
   "source": [
    "# Model 2: Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "df52e268-b8ff-4ee4-85aa-cb82c4e51bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbt_model = GradientBoostingClassifier()\n",
    "# Fit model \n",
    "gbt_model.fit(X_train, y_train)\n",
    "# Predict \n",
    "y_pred = gbt_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d11cb9d9-4976-455e-b458-6adfc66778cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9957828149710068\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2749b4ef-aed6-4bc7-bf0c-4b3a447a9ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       991\n",
      "           1       1.00      0.99      1.00       906\n",
      "\n",
      "    accuracy                           1.00      1897\n",
      "   macro avg       1.00      1.00      1.00      1897\n",
      "weighted avg       1.00      1.00      1.00      1897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f4da19d-82b8-4823-aa00-d6e4eb030c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_conf_matrix(y_pred, y_test, title=\"Gradient Boosting Classifier Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0b6d842e-fadb-4844-b25e-74dcb5552717",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "[CV 2/5; 3/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 2/5; 3/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=1.0;, score=0.508 total time=   0.5s\n",
      "[CV 3/5; 5/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 3/5; 5/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.75;, score=0.975 total time=   2.0s\n",
      "[CV 3/5; 7/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 3/5; 7/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.5;, score=0.975 total time=   3.3s\n",
      "[CV 5/5; 9/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 5/5; 9/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=1.0;, score=0.978 total time=   5.5s\n",
      "[CV 2/5; 16/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 2/5; 16/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.5;, score=0.982 total time=   3.1s\n",
      "[CV 4/5; 18/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 4/5; 18/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=1.0;, score=0.970 total time=   5.2s\n",
      "[CV 4/5; 24/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 4/5; 24/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=1.0;, score=0.970 total time=   2.9s\n",
      "[CV 2/5; 27/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 2/5; 27/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=1.0;, score=0.990 total time=   5.3s\n",
      "[CV 4/5; 32/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 4/5; 32/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.75;, score=0.970 total time=   2.2s\n",
      "[CV 5/5; 34/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 5/5; 34/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.5;, score=0.979 total time=   3.5s\n",
      "[CV 2/5; 37/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 2/5; 37/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.5;, score=0.990 total time=   0.3s\n",
      "[CV 4/5; 37/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 4/5; 37/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.5;, score=0.983 total time=   0.3s\n",
      "[CV 1/5; 38/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 1/5; 38/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.75;, score=0.975 total time=   0.4s\n",
      "[CV 3/5; 38/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 3/5; 38/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.75;, score=0.986 total time=   0.7s\n",
      "[CV 3/5; 39/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 3/5; 39/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=1.0;, score=0.975 total time=   0.8s\n",
      "[CV 1/5; 41/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 1/5; 41/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.75;, score=0.986 total time=   3.0s\n",
      "[CV 4/5; 43/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 4/5; 43/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.5;, score=0.992 total time=   3.9s\n",
      "[CV 1/5; 46/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 1/5; 46/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.5;, score=0.975 total time=   0.3s\n",
      "[CV 3/5; 46/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 3/5; 46/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.5;, score=0.975 total time=   0.3s\n",
      "[CV 4/5; 46/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 4/5; 46/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.5;, score=0.981 total time=   0.3s\n",
      "[CV 1/5; 47/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 1/5; 47/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.75;, score=0.975 total time=   0.6s\n",
      "[CV 3/5; 47/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 3/5; 47/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=0.75;, score=0.975 total time=   0.5s\n",
      "[CV 2/5; 48/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 2/5; 48/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=1.0;, score=0.979 total time=   0.6s\n",
      "[CV 4/5; 49/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 4/5; 49/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.5;, score=0.989 total time=   2.1s\n",
      "[CV 4/5; 51/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 4/5; 51/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=1.0;, score=0.990 total time=   3.6s\n",
      "[CV 1/5; 54/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 1/5; 54/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=1.0;, score=0.988 total time=   6.1s\n",
      "[CV 3/5; 59/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 3/5; 59/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=0.75;, score=0.996 total time=   2.8s\n",
      "[CV 5/5; 61/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 5/5; 61/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=0.5;, score=0.995 total time=   3.4s\n",
      "[CV 1/5; 64/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 1/5; 64/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=0.5;, score=0.988 total time=   0.3s\n",
      "[CV 2/5; 64/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 2/5; 64/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=0.5;, score=0.994 total time=   0.5s\n",
      "[CV 4/5; 64/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 4/5; 64/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=0.5;, score=0.989 total time=   0.4s\n",
      "[CV 1/5; 65/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 1/5; 65/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=0.75;, score=0.986 total time=   0.5s\n",
      "[CV 3/5; 65/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 3/5; 65/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=0.75;, score=0.993 total time=   0.5s\n",
      "[CV 2/5; 66/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 2/5; 66/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=1.0;, score=0.993 total time=   0.6s\n",
      "[CV 1/5; 67/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 1/5; 67/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=0.5;, score=0.986 total time=   1.9s\n",
      "[CV 1/5; 69/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 1/5; 69/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=1.0;, score=0.988 total time=   2.9s\n",
      "[CV 3/5; 71/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 3/5; 71/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=0.75;, score=0.995 total time=   5.4s\n",
      "[CV 5/5; 75/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 5/5; 75/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=1.0;, score=0.992 total time=   0.6s\n",
      "[CV 1/5; 77/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 1/5; 77/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=0.75;, score=0.982 total time=   2.7s\n",
      "[CV 5/5; 79/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=0.5[CV 2/5; 1/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 2/5; 1/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.5;, score=0.508 total time=   0.3s\n",
      "[CV 3/5; 3/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 3/5; 3/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=1.0;, score=0.507 total time=   0.5s\n",
      "[CV 5/5; 5/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 5/5; 5/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.75;, score=0.978 total time=   2.1s\n",
      "[CV 5/5; 7/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 5/5; 7/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.5;, score=0.978 total time=   3.8s\n",
      "[CV 3/5; 10/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 3/5; 10/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.5;, score=0.507 total time=   0.3s\n",
      "[CV 5/5; 10/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.5\n",
      "[CV 5/5; 10/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.5;, score=0.507 total time=   0.3s\n",
      "[CV 2/5; 11/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 2/5; 11/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.75;, score=0.508 total time=   0.4s\n",
      "[CV 5/5; 11/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 5/5; 11/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.75;, score=0.507 total time=   0.4s\n",
      "[CV 5/5; 12/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 5/5; 12/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=1.0;, score=0.507 total time=   0.5s\n",
      "[CV 5/5; 13/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 5/5; 13/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.5;, score=0.978 total time=   1.4s\n",
      "[CV 4/5; 14/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 4/5; 14/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.75;, score=0.970 total time=   2.1s\n",
      "[CV 5/5; 16/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 5/5; 16/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.5;, score=0.978 total time=   3.6s\n",
      "[CV 4/5; 19/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 4/5; 19/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.5;, score=0.970 total time=   0.3s\n",
      "[CV 1/5; 20/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 1/5; 20/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.75;, score=0.975 total time=   0.5s\n",
      "[CV 3/5; 20/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 3/5; 20/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.75;, score=0.975 total time=   0.5s\n",
      "[CV 3/5; 21/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 3/5; 21/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=1.0;, score=0.975 total time=   0.5s\n",
      "[CV 2/5; 22/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 2/5; 22/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.5;, score=0.990 total time=   1.8s\n",
      "[CV 4/5; 23/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 4/5; 23/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.75;, score=0.970 total time=   2.7s\n",
      "[CV 3/5; 26/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 3/5; 26/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.75;, score=0.986 total time=   4.5s\n",
      "[CV 2/5; 30/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 2/5; 30/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=1.0;, score=0.980 total time=   0.7s\n",
      "[CV 4/5; 31/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 4/5; 31/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.5;, score=0.971 total time=   1.6s\n",
      "[CV 4/5; 33/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 4/5; 33/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=1.0;, score=0.970 total time=   3.0s\n",
      "[CV 2/5; 36/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=1.0\n",
      "[CV 2/5; 36/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=1.0;, score=0.979 total time=   5.8s\n",
      "[CV 2/5; 42/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 2/5; 42/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=1.0;, score=0.996 total time=   3.2s\n",
      "[CV 4/5; 44/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 4/5; 44/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.75;, score=0.991 total time=   4.7s\n",
      "[CV 1/5; 49/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 1/5; 49/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.5;, score=0.984 total time=   2.0s\n",
      "[CV 1/5; 51/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 1/5; 51/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=1.0;, score=0.986 total time=   3.1s\n",
      "[CV 3/5; 53/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 3/5; 53/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.75;, score=0.993 total time=   4.9s\n",
      "[CV 2/5; 57/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 2/5; 57/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=1.0;, score=0.992 total time=   0.7s\n",
      "[CV 4/5; 58/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 4/5; 58/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=0.5;, score=0.984 total time=   2.3s\n",
      "[CV 3/5; 60/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 3/5; 60/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=1.0;, score=0.994 total time=   3.1s\n",
      "[CV 5/5; 62/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 5/5; 62/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=0.75;, score=0.993 total time=   4.9s\n",
      "[CV 3/5; 67/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 3/5; 67/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=0.5;, score=0.996 total time=   1.8s\n",
      "[CV 2/5; 69/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 2/5; 69/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=1.0;, score=0.996 total time=   3.1s\n",
      "[CV 4/5; 71/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 4/5; 71/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=0.75;, score=0.992 total time=   5.2s\n",
      "[CV 2/5; 76/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 2/5; 76/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=0.5;, score=0.993 total time=   1.7s\n",
      "[CV 4/5; 77/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 4/5; 77/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=0.75;, score=0.988 total time=   2.4s\n",
      "[CV 1/5; 80/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 1/5; 80/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=0.75;, score=0.986 total time=   4.3s\n",
      "[CV 4/5; 83/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 4/5; 83/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=0.75;, score=0.987 total time=   0.6s\n",
      "[CV 4/5; 84/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=1.0[CV 5/5; 2/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 5/5; 2/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.75;, score=0.507 total time=   0.5s\n",
      "[CV 2/5; 5/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 2/5; 5/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.75;, score=0.980 total time=   2.3s\n",
      "[CV 1/5; 8/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 1/5; 8/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=0.75;, score=0.983 total time=   4.7s\n",
      "[CV 4/5; 11/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 4/5; 11/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=0.75;, score=0.507 total time=   0.4s\n",
      "[CV 1/5; 12/90] START learning_rate=0.001, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 1/5; 12/90] END learning_rate=0.001, loss=exponential, n_estimators=10, subsample=1.0;, score=0.507 total time=   0.6s\n",
      "[CV 2/5; 13/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 2/5; 13/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.5;, score=0.994 total time=   1.6s\n",
      "[CV 5/5; 14/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 5/5; 14/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.75;, score=0.978 total time=   2.2s\n",
      "[CV 2/5; 17/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 2/5; 17/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.75;, score=0.980 total time=   4.8s\n",
      "[CV 2/5; 21/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 2/5; 21/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=1.0;, score=0.980 total time=   0.5s\n",
      "[CV 3/5; 22/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 3/5; 22/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.5;, score=0.975 total time=   1.7s\n",
      "[CV 2/5; 23/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 2/5; 23/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.75;, score=0.990 total time=   2.4s\n",
      "[CV 1/5; 26/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 1/5; 26/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.75;, score=0.982 total time=   4.7s\n",
      "[CV 3/5; 29/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 3/5; 29/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=0.75;, score=0.975 total time=   0.5s\n",
      "[CV 4/5; 30/90] START learning_rate=0.01, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 4/5; 30/90] END learning_rate=0.01, loss=exponential, n_estimators=10, subsample=1.0;, score=0.970 total time=   0.7s\n",
      "[CV 3/5; 32/90] START learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 3/5; 32/90] END learning_rate=0.01, loss=exponential, n_estimators=50, subsample=0.75;, score=0.975 total time=   2.7s\n",
      "[CV 2/5; 35/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 2/5; 35/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.75;, score=0.979 total time=   4.6s\n",
      "[CV 1/5; 39/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 1/5; 39/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=1.0;, score=0.971 total time=   0.5s\n",
      "[CV 5/5; 39/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 5/5; 39/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=1.0;, score=0.993 total time=   0.5s\n",
      "[CV 5/5; 40/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 5/5; 40/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.5;, score=0.996 total time=   1.7s\n",
      "[CV 3/5; 42/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 3/5; 42/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=1.0;, score=0.994 total time=   3.2s\n",
      "[CV 1/5; 45/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 1/5; 45/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=1.0;, score=0.988 total time=   6.7s\n",
      "[CV 5/5; 50/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 5/5; 50/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.75;, score=0.995 total time=   2.6s\n",
      "[CV 1/5; 53/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 1/5; 53/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.75;, score=0.988 total time=   5.4s\n",
      "[CV 1/5; 57/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 1/5; 57/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=1.0;, score=0.988 total time=   0.7s\n",
      "[CV 3/5; 58/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 3/5; 58/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=0.5;, score=0.994 total time=   2.0s\n",
      "[CV 5/5; 59/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 5/5; 59/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=0.75;, score=0.996 total time=   2.2s\n",
      "[CV 1/5; 62/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 1/5; 62/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=0.75;, score=0.989 total time=   5.2s\n",
      "[CV 4/5; 65/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 4/5; 65/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=0.75;, score=0.989 total time=   0.5s\n",
      "[CV 3/5; 66/90] START learning_rate=1, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 3/5; 66/90] END learning_rate=1, loss=exponential, n_estimators=10, subsample=1.0;, score=0.993 total time=   0.6s\n",
      "[CV 4/5; 67/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 4/5; 67/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=0.5;, score=0.993 total time=   1.6s\n",
      "[CV 5/5; 68/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 5/5; 68/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=0.75;, score=0.996 total time=   3.0s\n",
      "[CV 2/5; 71/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 2/5; 71/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=0.75;, score=0.996 total time=   5.3s\n",
      "[CV 4/5; 75/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 4/5; 75/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=1.0;, score=0.994 total time=   0.7s\n",
      "[CV 5/5; 76/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 5/5; 76/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=0.5;, score=0.987 total time=   1.7s\n",
      "[CV 4/5; 78/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 4/5; 78/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=1.0;, score=0.988 total time=   3.0s\n",
      "[CV 1/5; 81/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 1/5; 81/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=1.0;, score=0.988 total time=   3.0s\n",
      "[CV 2/5; 83/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=0.75\n",
      "[CV 2/5; 83/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=0.75;, score=0.993 total time=   0.6s\n",
      "[CV 1/5; 84/90] START learning_rate=2, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 1/5; 84/90] END learning_rate=2, loss=exponential, n_estimators=10, subsample=1.0;, score=0.984 total time=   0.6s\n",
      "[CV 1/5; 85/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 1/5; 85/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=0.5;, score=0.984 total time=   1.5s\n",
      "[CV 4/5; 86/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=0.75[CV 4/5; 2/90] START learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 4/5; 2/90] END learning_rate=0.001, loss=deviance, n_estimators=10, subsample=0.75;, score=0.507 total time=   0.5s\n",
      "[CV 5/5; 4/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 5/5; 4/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=0.5;, score=0.978 total time=   1.5s\n",
      "[CV 4/5; 6/90] START learning_rate=0.001, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 4/5; 6/90] END learning_rate=0.001, loss=deviance, n_estimators=50, subsample=1.0;, score=0.970 total time=   2.5s\n",
      "[CV 1/5; 9/90] START learning_rate=0.001, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 1/5; 9/90] END learning_rate=0.001, loss=deviance, n_estimators=100, subsample=1.0;, score=0.986 total time=   5.3s\n",
      "[CV 3/5; 14/90] START learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 3/5; 14/90] END learning_rate=0.001, loss=exponential, n_estimators=50, subsample=0.75;, score=0.975 total time=   2.4s\n",
      "[CV 1/5; 17/90] START learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 1/5; 17/90] END learning_rate=0.001, loss=exponential, n_estimators=100, subsample=0.75;, score=0.971 total time=   4.7s\n",
      "[CV 5/5; 20/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 5/5; 20/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=0.75;, score=0.978 total time=   0.4s\n",
      "[CV 4/5; 21/90] START learning_rate=0.01, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 4/5; 21/90] END learning_rate=0.01, loss=deviance, n_estimators=10, subsample=1.0;, score=0.970 total time=   0.5s\n",
      "[CV 4/5; 22/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 4/5; 22/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.5;, score=0.971 total time=   1.5s\n",
      "[CV 5/5; 23/90] START learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 5/5; 23/90] END learning_rate=0.01, loss=deviance, n_estimators=50, subsample=0.75;, score=0.982 total time=   2.2s\n",
      "[CV 5/5; 25/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.5\n",
      "[CV 5/5; 25/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=0.5;, score=0.990 total time=   3.0s\n",
      "[CV 5/5; 27/90] START learning_rate=0.01, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 5/5; 27/90] END learning_rate=0.01, loss=deviance, n_estimators=100, subsample=1.0;, score=0.990 total time=   5.6s\n",
      "[CV 1/5; 35/90] START learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 1/5; 35/90] END learning_rate=0.01, loss=exponential, n_estimators=100, subsample=0.75;, score=0.968 total time=   4.7s\n",
      "[CV 5/5; 38/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 5/5; 38/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=0.75;, score=0.990 total time=   0.4s\n",
      "[CV 4/5; 39/90] START learning_rate=0.1, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 4/5; 39/90] END learning_rate=0.1, loss=deviance, n_estimators=10, subsample=1.0;, score=0.983 total time=   0.5s\n",
      "[CV 3/5; 40/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 3/5; 40/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.5;, score=0.994 total time=   1.7s\n",
      "[CV 5/5; 41/90] START learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.75\n",
      "[CV 5/5; 41/90] END learning_rate=0.1, loss=deviance, n_estimators=50, subsample=0.75;, score=0.996 total time=   2.5s\n",
      "[CV 2/5; 44/90] START learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 2/5; 44/90] END learning_rate=0.1, loss=deviance, n_estimators=100, subsample=0.75;, score=0.996 total time=   5.2s\n",
      "[CV 3/5; 48/90] START learning_rate=0.1, loss=exponential, n_estimators=10, subsample=1.0\n",
      "[CV 3/5; 48/90] END learning_rate=0.1, loss=exponential, n_estimators=10, subsample=1.0;, score=0.982 total time=   0.6s\n",
      "[CV 3/5; 49/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 3/5; 49/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=0.5;, score=0.992 total time=   2.0s\n",
      "[CV 2/5; 51/90] START learning_rate=0.1, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 2/5; 51/90] END learning_rate=0.1, loss=exponential, n_estimators=50, subsample=1.0;, score=0.993 total time=   3.3s\n",
      "[CV 4/5; 53/90] START learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 4/5; 53/90] END learning_rate=0.1, loss=exponential, n_estimators=100, subsample=0.75;, score=0.990 total time=   5.0s\n",
      "[CV 5/5; 57/90] START learning_rate=1, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 5/5; 57/90] END learning_rate=1, loss=deviance, n_estimators=10, subsample=1.0;, score=0.993 total time=   0.5s\n",
      "[CV 5/5; 58/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 5/5; 58/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=0.5;, score=0.991 total time=   2.2s\n",
      "[CV 4/5; 60/90] START learning_rate=1, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 4/5; 60/90] END learning_rate=1, loss=deviance, n_estimators=50, subsample=1.0;, score=0.990 total time=   3.4s\n",
      "[CV 1/5; 63/90] START learning_rate=1, loss=deviance, n_estimators=100, subsample=1.0\n",
      "[CV 1/5; 63/90] END learning_rate=1, loss=deviance, n_estimators=100, subsample=1.0;, score=0.989 total time=   5.8s\n",
      "[CV 3/5; 68/90] START learning_rate=1, loss=exponential, n_estimators=50, subsample=0.75\n",
      "[CV 3/5; 68/90] END learning_rate=1, loss=exponential, n_estimators=50, subsample=0.75;, score=0.995 total time=   2.3s\n",
      "[CV 4/5; 70/90] START learning_rate=1, loss=exponential, n_estimators=100, subsample=0.5\n",
      "[CV 4/5; 70/90] END learning_rate=1, loss=exponential, n_estimators=100, subsample=0.5;, score=0.990 total time=   3.7s\n",
      "[CV 1/5; 73/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 1/5; 73/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=0.5;, score=0.978 total time=   0.4s\n",
      "[CV 2/5; 73/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 2/5; 73/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=0.5;, score=0.994 total time=   0.4s\n",
      "[CV 5/5; 73/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=0.5\n",
      "[CV 5/5; 73/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=0.5;, score=0.991 total time=   0.5s\n",
      "[CV 2/5; 74/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 2/5; 74/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=0.75;, score=0.994 total time=   0.4s\n",
      "[CV 4/5; 74/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=0.75\n",
      "[CV 4/5; 74/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=0.75;, score=0.988 total time=   0.5s\n",
      "[CV 2/5; 75/90] START learning_rate=2, loss=deviance, n_estimators=10, subsample=1.0\n",
      "[CV 2/5; 75/90] END learning_rate=2, loss=deviance, n_estimators=10, subsample=1.0;, score=0.995 total time=   0.6s\n",
      "[CV 1/5; 76/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=0.5\n",
      "[CV 1/5; 76/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=0.5;, score=0.986 total time=   1.8s\n",
      "[CV 1/5; 78/90] START learning_rate=2, loss=deviance, n_estimators=50, subsample=1.0\n",
      "[CV 1/5; 78/90] END learning_rate=2, loss=deviance, n_estimators=50, subsample=1.0;, score=0.988 total time=   2.5s\n",
      "[CV 2/5; 80/90] START learning_rate=2, loss=deviance, n_estimators=100, subsample=0.75\n",
      "[CV 2/5; 80/90] END learning_rate=2, loss=deviance, n_estimators=100, subsample=0.75;, score=0.995 total time=   5.4s\n",
      "[CV 5/5; 85/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=0.5\n",
      "[CV 5/5; 85/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=0.5;, score=0.996 total time=   1.7s\n",
      "[CV 5/5; 87/90] START learning_rate=2, loss=exponential, n_estimators=50, subsample=1.0\n",
      "[CV 5/5; 87/90] END learning_rate=2, loss=exponential, n_estimators=50, subsample=1.0;, score=0.995 total time=   2.9s\n",
      "[CV 4/5; 89/90] START learning_rate=2, loss=exponential, n_estimators=100, subsample=0.75\n",
      "[CV 4/5; 89/90] END learning_rate=2, loss=exponential, n_estimators=100, subsample=0.75;, score=0.989 total time=   4.7sCPU times: user 1.75 s, sys: 161 ms, total: 1.91 s\n",
      "Wall time: 1min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    \"loss\": [\"deviance\", \"exponential\"],\n",
    "    \"learning_rate\": [0.001, 0.01, 0.1, 1, 2],\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"subsample\": [0.5, 0.75, 1.0],\n",
    "}\n",
    "\n",
    "gs_clf = GridSearchCV(GradientBoostingClassifier(), params, n_jobs=-1, scoring='accuracy', verbose=10)\n",
    "  \n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "# Predict \n",
    "y_pred = gs_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d5ad5e24-2396-4ac2-83c4-0f31188248c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9947285187137586\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c0e3933b-738c-4180-af2e-95b10b6ee6c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       991\n",
      "           1       1.00      0.99      0.99       906\n",
      "\n",
      "    accuracy                           0.99      1897\n",
      "   macro avg       1.00      0.99      0.99      1897\n",
      "weighted avg       0.99      0.99      0.99      1897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f4431dc-c563-47a8-b3e0-670c98ce43d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_conf_matrix(y_pred, y_test, title=\"Gradient Boosting Classifier Classifier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a16e17f-78a2-4ec5-970f-32f775bbadc5",
   "metadata": {},
   "source": [
    "# Model 3: Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a33153a4-84bc-4f96-a5a7-9997b4f8eae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier()\n",
    "# Fit model \n",
    "rf_model.fit(X_train, y_train)\n",
    "# Predict \n",
    "y_pred = rf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7a541c4-90ed-41a6-9300-bdc58a773dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"n_estimators\": [10, 50, 100, 200],\n",
    "    \"min_samples_leaf\": [1, 2, 4],\n",
    "}\n",
    "gs_clf = GridSearchCV(RandomForestClassifier(), params, n_jobs=-1)\n",
    "gs_clf = gs_clf.fit(X_train, y_train)\n",
    "# Predict \n",
    "y_pred = gs_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1bd442e5-089d-49eb-be7c-02d3cf59400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9942013705851345\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dca79d09-51fb-4d2a-b754-ff11a7e43e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       991\n",
      "           1       1.00      0.99      0.99       906\n",
      "\n",
      "    accuracy                           0.99      1897\n",
      "   macro avg       0.99      0.99      0.99      1897\n",
      "weighted avg       0.99      0.99      0.99      1897\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e35290f3-6c1a-40fe-b990-56e90dd4516f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_conf_matrix(y_pred, y_test, title=\"Random Forest Classifier Classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9e4671-a574-468d-8c72-021aabf2f896",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
